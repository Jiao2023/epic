{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaopanyu/.conda/envs/chemprop/lib/python3.8/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.27' not found (required by /home/jiaopanyu/.conda/envs/chemprop/lib/python3.8/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/jiaopanyu/.conda/envs/chemprop/lib/python3.8/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/jiaopanyu/.conda/envs/chemprop/lib/python3.8/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union, Tuple\n",
    "from functools import reduce\n",
    "from torch_geometric import nn as pnn\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_scatter import scatter_sum, scatter_mean\n",
    "from torch_geometric.nn.norm.batch_norm import BatchNorm\n",
    "import time\n",
    "from torch_geometric.nn.models import MLP,GCN,GAT,GraphSAGE,GIN,BasicGNN\n",
    "import networkx as nx\n",
    "import math\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from typing import Any, List, Optional, Tuple,Final\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import cumsum, to_dense_batch, scatter\n",
    "from torch_geometric.typing import OptTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from typing import Callable, Optional, Union\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    Size,\n",
    "    SparseTensor,\n",
    ")\n",
    "from torch_geometric.utils import spmm\n",
    "from torch_geometric.nn.dense import DenseGINConv\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_atoms = torch.load('/home/jiaopanyu/save_components/f_atoms.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_atoms.shape,f_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bonds = torch.load('/home/jiaopanyu/save_components/f_bonds.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bonds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_map = torch.load('/home/jiaopanyu/save_components/f_map.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f_map),f_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_xyz = torch.load('/home/jiaopanyu/save_components/f_xyz.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_xyz.shape,f_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_scope = torch.load('/home/jiaopanyu/save_components/a_scope.pth')\n",
    "b_scope = torch.load('/home/jiaopanyu/save_components/b_scope.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b = torch.load('/home/jiaopanyu/save_components/a2b.pth')\n",
    "b2a = torch.load('/home/jiaopanyu/save_components/b2a.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2revb = torch.load('/home/jiaopanyu/save_components/b2revb.pth')\n",
    "b2revb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_sequence(input_dim, output_dim, n_hidden):\n",
    "    \"\"\"\n",
    "    Smoothly decay the number of hidden units in each layer.\n",
    "    Start from 'input_dim' and end with 'output_dim'.\n",
    "  \n",
    "    Examples:\n",
    "    get_unit_sequence(1,1024,4) = [1, 4, 16, 64, 256, 1024]\n",
    "    get_unit_sequence(1024,1,4) = [1024, 256, 64, 16, 4, 1]\n",
    "    \"\"\"\n",
    "    reverse = False\n",
    "    if input_dim > output_dim:\n",
    "        reverse = True\n",
    "        input_dim,output_dim = output_dim,input_dim\n",
    "\n",
    "    diff = abs(output_dim.bit_length() - input_dim.bit_length())\n",
    "    increment = diff // (n_hidden+1)\n",
    "\n",
    "    sequence = [input_dim] + [0] * (n_hidden) + [output_dim]\n",
    "\n",
    "    for idx in range(n_hidden // 2):\n",
    "        sequence[idx+1] = 2 ** ((sequence[idx]).bit_length() + increment-1)\n",
    "        sequence[-2-idx] = 2 ** ((sequence[-1-idx]-1).bit_length() - increment)\n",
    "\n",
    "    if n_hidden%2 == 1:\n",
    "        sequence[n_hidden // 2 + 1] = (sequence[n_hidden // 2] + sequence[n_hidden // 2+2])//2\n",
    "\n",
    "    if reverse: \n",
    "        sequence.reverse()\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden(nn.Module):\n",
    "    def __init__(self, size_in, size_out, activation):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(size_in, size_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ffn(nn.Module):\n",
    "    \"\"\"\n",
    "    A Feed-Forward neural Network that uses DenseHidden layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, capacity, activation):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.unit_sequence = get_unit_sequence(\n",
    "            input_dim, output_dim, capacity\n",
    "        )\n",
    "        # set up hidden layers\n",
    "        for ind, n_units in enumerate(self.unit_sequence[:-1]):\n",
    "            size_out_ = self.unit_sequence[ind + 1]\n",
    "            self.layers.append(\n",
    "                hidden(\n",
    "                    size_in=n_units,\n",
    "                    size_out=size_out_,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of this model\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model:polygin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaopanyu/.conda/envs/chemprop/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chempropfix.args import TrainArgs\n",
    "from chempropfix.nn_utils import get_activation_function\n",
    "from chempropfix.models.mpn import GAIN,MPNEncoder,torch_adj_helper,mol_graph2data\n",
    "from chempropfix.features import BatchMolGraph, get_atom_fdim, get_bond_fdim, mol2graph\n",
    "from chempropfix.models.model import MoleculeModel\n",
    "from chempropfix.models.egnn import E_GCL_mask,EGNN\n",
    "from chempropfix.models.gcl import E_GCL, unsorted_segment_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POLYGINConv(MessagePassing):\n",
    "    r\"\"\"The graph isomorphism operator from the `\"How Powerful are\n",
    "    Graph Neural Networks?\" <https://arxiv.org/abs/1810.00826>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = h_{\\mathbf{\\Theta}} \\left( (1 + \\epsilon) \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j \\right)\n",
    "\n",
    "    or\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = h_{\\mathbf{\\Theta}} \\left( \\left( \\mathbf{A} +\n",
    "        (1 + \\epsilon) \\cdot \\mathbf{I} \\right) \\cdot \\mathbf{X} \\right),\n",
    "\n",
    "    here :math:`h_{\\mathbf{\\Theta}}` denotes a neural network, *.i.e.* an MLP.\n",
    "\n",
    "    Args:\n",
    "        nn (torch.nn.Module): A neural network :math:`h_{\\mathbf{\\Theta}}` that\n",
    "            maps node features :obj:`x` of shape :obj:`[-1, in_channels]` to\n",
    "            shape :obj:`[-1, out_channels]`, *e.g.*, defined by\n",
    "            :class:`torch.nn.Sequential`.\n",
    "        eps (float, optional): (Initial) :math:`\\epsilon`-value.\n",
    "            (default: :obj:`0.`)\n",
    "        train_eps (bool, optional): If set to :obj:`True`, :math:`\\epsilon`\n",
    "            will be a trainable parameter. (default: :obj:`False`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **input:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})` or\n",
    "          :math:`((|\\mathcal{V_s}|, F_{s}), (|\\mathcal{V_t}|, F_{t}))`\n",
    "          if bipartite,\n",
    "          edge indices :math:`(2, |\\mathcal{E}|)`\n",
    "        - **output:** node features :math:`(|\\mathcal{V}|, F_{out})` or\n",
    "          :math:`(|\\mathcal{V}_t|, F_{out})` if bipartite\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,out_channels, nn: Callable, eps: float = 0., train_eps: bool = False, node_hidden: Optional[int] = None, edge_hidden: Optional[int] = None,activation:Callable=None,\n",
    "                 **kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "        self.edge_mapper = ffn(edge_hidden, edge_hidden,capacity=1,activation=activation)\n",
    "        self.residual_mapping = ffn(in_channels+edge_hidden, in_channels,capacity=1,activation=activation)\n",
    "        # self.node_mapper = ffn(in_channels, in_channels,capacity=1,activation=activation)\n",
    "        self.nn = nn\n",
    "        self.initial_eps = eps\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.empty(1))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.empty(1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        reset(self.nn)\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,edge_attr: Union[Tensor, OptPairTensor],\n",
    "                size: Size = None) -> Tensor:\n",
    "      \n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        out = self.propagate(edge_index,size=size,x=x,edge_attr=edge_attr)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if x_r is not None:\n",
    "            out = out + (1 + self.eps) * x_r\n",
    "        return self.nn(out)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        # x_j = self.node_mapper(x_j)\n",
    "        edge_attr = self.edge_mapper(edge_attr)\n",
    "        return self.residual_mapping(torch.concat((x_j,edge_attr),dim=-1))\n",
    "\n",
    "    # def message_and_aggregate(self, adj_t: SparseTensor,\n",
    "    #                           x: OptPairTensor) -> Tensor:\n",
    "    #     if isinstance(adj_t, SparseTensor):\n",
    "    #         adj_t = adj_t.set_value(None, layout=None)\n",
    "    #     return spmm(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(nn={self.nn})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POLYGIN(BasicGNN):\n",
    "    r\"\"\"The Graph Neural Network from the `\"How Powerful are Graph Neural\n",
    "    Networks?\" <https://arxiv.org/abs/1810.00826>`_ paper, using the\n",
    "    :class:`~torch_geometric.nn.GINConv` operator for message passing.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        hidden_channels (int): Size of each hidden sample.\n",
    "        num_layers (int): Number of message passing layers.\n",
    "        out_channels (int, optional): If not set to :obj:`None`, will apply a\n",
    "            final linear transformation to convert hidden node embeddings to\n",
    "            output size :obj:`out_channels`. (default: :obj:`None`)\n",
    "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
    "        act (str or Callable, optional): The non-linear activation function to\n",
    "            use. (default: :obj:`\"relu\"`)\n",
    "        act_first (bool, optional): If set to :obj:`True`, activation is\n",
    "            applied before normalization. (default: :obj:`False`)\n",
    "        act_kwargs (Dict[str, Any], optional): Arguments passed to the\n",
    "            respective activation function defined by :obj:`act`.\n",
    "            (default: :obj:`None`)\n",
    "        norm (str or Callable, optional): The normalization function to\n",
    "            use. (default: :obj:`None`)\n",
    "        norm_kwargs (Dict[str, Any], optional): Arguments passed to the\n",
    "            respective normalization function defined by :obj:`norm`.\n",
    "            (default: :obj:`None`)\n",
    "        jk (str, optional): The Jumping Knowledge mode. If specified, the model\n",
    "            will additionally apply a final linear transformation to transform\n",
    "            node embeddings to the expected output feature dimensionality.\n",
    "            (:obj:`None`, :obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`,\n",
    "            :obj:`\"lstm\"`). (default: :obj:`None`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.GINConv`.\n",
    "    \"\"\"\n",
    "    supports_edge_weight: Final[bool] = False\n",
    "    supports_edge_attr: Final[bool] = True\n",
    "    supports_norm_batch: Final[bool]\n",
    "\n",
    "    def init_conv(self, in_channels: int, out_channels: int,\n",
    "                  **kwargs) -> MessagePassing:\n",
    "        mlp = MLP(\n",
    "            [in_channels,in_channels, out_channels],\n",
    "            act=self.act,\n",
    "            act_first=self.act_first,\n",
    "            norm=self.norm,\n",
    "            norm_kwargs=self.norm_kwargs,\n",
    "        )\n",
    "        return POLYGINConv(in_channels,out_channels,mlp, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_interleave(\n",
    "    repeats: List[int],\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Tensor:\n",
    "    outs = [torch.full((n, ), i, device=device) for i, n in enumerate(repeats)]\n",
    "    return torch.cat(outs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mol_graph2data(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(mol_graph2data,self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self,\n",
    "        mol_graph: BatchMolGraph,\n",
    "        atom_descriptors_batch: List[np.ndarray] = None) -> torch.FloatTensor:\n",
    "        \n",
    "    \n",
    "        f_atoms, f_bonds,a2b, b2a, b2revb, a_scope, b_scope,f_map,f_xyz = mol_graph.get_components()\n",
    "        print(f_atoms.shape)\n",
    "        print(f_bonds.shape)\n",
    "        print(a2b.shape)\n",
    "        print(b2a.shape)\n",
    "        print(b2revb.shape)\n",
    "        print(a_scope.shape)\n",
    "        print(b_scope.shape)\n",
    "        print(f_map.shape)\n",
    "        print(f_xyz.shape)\n",
    "\n",
    "\n",
    "        repeats = [i[1] for i in a_scope]  # 原子数\n",
    "        batch = repeat_interleave(repeats, device=self.args.device)\n",
    "        ptr = cumsum(torch.tensor(repeats, device=self.args.device))\n",
    "        b2revb_individual = []\n",
    "        edge_index = [[],[]]\n",
    "        for idx, (a_scope_i, b_scope_i) in enumerate(zip(a_scope, b_scope)):\n",
    "            b2a_i = b2a[b_scope_i[0]:b_scope_i[0] + b_scope_i[1]]\n",
    "            b2revb_i = b2revb[b_scope_i[0]:b_scope_i[0] + b_scope_i[1]]\n",
    "            edge_index[0].extend((b2a_i).tolist())\n",
    "            edge_index[1].extend((b2a_i[b2revb_i - b_scope_i[0]]).tolist())\n",
    "            # 如果使用pyG，那么就不需要b2revb_individual\n",
    "            # b2revb_individual.append(b2revb[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]]-b_scope_i[0])\n",
    "            # b2revb_individual = torch.cat(b2revb_individual,dim=0)\n",
    "        edge_index = torch.tensor(edge_index,dtype=torch.long,device=self.args.device)-1    #因为chemprop内部的设置，这里必须-1\n",
    "        return f_atoms[1:,:].to(self.args.device),\\\n",
    "               edge_index, \\\n",
    "               f_bonds[1:,:].to(self.args.device),\\\n",
    "               b2revb_individual, \\\n",
    "               batch, ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pyG_helper(nn.Module):\n",
    "    def __init__(self, args: TrainArgs, atom_fdim: int, bond_fdim: int):\n",
    "\n",
    "        super(pyG_helper, self).__init__()\n",
    "        # 创建数据格式：mol_graph2data\n",
    "        self.mol_graph2data_layer = mol_graph2data(args)\n",
    "        self.args = args\n",
    "        self.encoder_type = args.encoder_type\n",
    "\n",
    "        if args.encoder_type in [\"gcn\",\"gcn_attn\",\"gcn_pe\"]:\n",
    "            self.mpn = GCN(in_channels=atom_fdim,hidden_channels=args.hidden_size,num_layers=args.depth,dropout=args.gnn_dropout,activation=get_activation_function(args.activation))\n",
    "        elif args.encoder_type in [\"gat\",\"gat_attn\",\"gat_pe\"]:\n",
    "            self.mpn = GAT(in_channels=atom_fdim,hidden_channels=args.hidden_size,num_layers=args.depth,dropout=args.gnn_dropout,activation=get_activation_function(args.activation))\n",
    "        elif args.encoder_type in [\"graphsage\",\"graphsage_attn\",\"graphsage_pe\"]:\n",
    "            self.mpn = GraphSAGE(in_channels=atom_fdim,hidden_channels=args.hidden_size,num_layers=args.depth,dropout=args.gnn_dropout,activation=get_activation_function(args.activation))\n",
    "        elif args.encoder_type in [\"gin\",\"gin_attn\",\"gin_pe\"]:\n",
    "            self.mpn = GIN(in_channels=atom_fdim,hidden_channels=args.hidden_size,num_layers=args.depth,dropout=args.gnn_dropout,act=get_activation_function(args.activation))\n",
    "        elif args.encoder_type in [\"polygnn\", \"polygnn_attn\", \"polygnn_pe\"]:\n",
    "            hps={\n",
    "                \"readout_dim\" : args.hidden_size,\n",
    "                \"depth\" : args.depth,\n",
    "                \"activation\" : get_activation_function(args.activation),\n",
    "                \"ffn_capacity\": 0,\n",
    "                \"dropout\":args.gnn_dropout,\n",
    "                \"visual\": 21\n",
    "            }\n",
    "            # self.mpn = polygnn_mp(node_size= atom_fdim, edge_size= bond_fdim,hps=hps,normalize_embedding=True)\n",
    "        elif args.encoder_type in [\"gain\", \"gain_attn\", \"gain_pe\"]:\n",
    "            self.mpn = GAIN(args, atom_fdim, bond_fdim)\n",
    "        #elif args.encoder_type in ['graphformer']:\n",
    "            #self.mpn = graphformerEncoder(args, atom_fdim, bond_fdim)\n",
    "        elif args.encoder_type in [\"polygin\", \"polygin_attn\"]:\n",
    "            # self.node_mapper = nn.Linear(atom_fdim,args.hidden_size)\n",
    "            self.mpn = POLYGIN(in_channels=atom_fdim,hidden_channels=args.hidden_size,num_layers=args.depth,dropout=args.gnn_dropout,activation=get_activation_function(args.activation),edge_hidden=bond_fdim,node_hidden=args.hidden_size)\n",
    "        #self.with_attn = args.with_attn\n",
    "        #if self.with_attn:\n",
    "        #    self.attention_encoder = AttnEncoderXL(args,bond_fdim)\n",
    "        # self.layer_norm  = nn.LayerNorm(args.hidden_size, eps=1e-6,elementwise_affine=True)\n",
    "\n",
    "    def forward(self,\n",
    "                mol_graph: BatchMolGraph,\n",
    "                atom_descriptors_batch: List[np.ndarray] = None) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        f_atoms[1:,:].to(self.args.device),\\\n",
    "               edge_index, \\\n",
    "               f_bonds[1:,:].to(self.args.device),\\\n",
    "               b2revb_individual, \\\n",
    "               batch, ptr\n",
    "        \"\"\"\n",
    "        # 做数据转化\n",
    "        x, edge_index, edge_attr, b2revb_individual, batch, ptr = self.mol_graph2data_layer(mol_graph) \n",
    "        \n",
    "        if self.args.encoder_type in [\"polygin\", \"polygin_attn\", \"polygin_pe\"]:\n",
    "            # x = self.node_mapper(x)\n",
    "            poly_vec = self.mpn(x=x, edge_index=edge_index, edge_attr=edge_attr, batch=batch)\n",
    "            # poly_vec = w_atoms.view(w_atoms.shape[0], 1) * poly_vec\n",
    "            # poly_vec = scatter_sum(poly_vec, batch, dim=0)\n",
    "            # w_atoms = scatter_sum(w_atoms, batch, dim=0)\n",
    "            # poly_vec = poly_vec / w_atoms.view(w_atoms.shape[0], 1)\n",
    "        #elif self.encoder_type in [\"polygnn\", \"polygnn_attn\", \"polygnn_pe\"]:\n",
    "            #poly_vec = self.mpn(x=x, edge_index=edge_index, edge_attr=edge_attr,batch=batch)\n",
    "        #elif self.encoder_type in [\"graphformer\"]:\n",
    "             #poly_vec = self.mpn(x, edge_index, edge_attr,ptr)\n",
    "        else:\n",
    "            poly_vec = self.mpn(x=x, edge_index=edge_index, edge_attr=edge_attr,batch=batch)\n",
    "           \n",
    "        poly_vec = scatter_sum(poly_vec, batch, dim=0)\n",
    "        #if self.args.visual == 21:\n",
    "            #return poly_vec\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.with_attn:\n",
    "            lengths = ptr[1:] - ptr[:-1]\n",
    "            dense_batch, mask = to_dense_batch(torch.arange(poly_vec.size(0),device=self.args.device), batch=batch)\n",
    "            mask_matrix = ~(mask.unsqueeze(2) & mask.unsqueeze(1))        # [batch_size, max_size] => [batch_size,max_size,max_size]\n",
    "            node_count = torch.bincount(batch)\n",
    "            node_features_split = torch.split(poly_vec, node_count.tolist())\n",
    "            max_length = dense_batch.size(1)\n",
    "            padded_tensor = torch.stack([torch.cat([feat, torch.zeros(max_length - feat.size(0), feat.size(1),device=self.args.device)]) for feat in node_features_split])\n",
    "\n",
    "            atom_message, attns = self.attention_encoder(poly_vec,padded_tensor,mask_matrix,edge_index,edge_attr,ptr,batch,distances,node_paths,edge_paths)\n",
    "            # atom_message = torch.cat([atom_message[i, :a,:] for i,a in enumerate(lengths)], dim=0)\n",
    "            attns = torch.sum(attns,dim=1)\n",
    "            if self.args.visual:\n",
    "                return attns\n",
    "                # torch.save(attns, '/home/chenlidong/polyAttn/notebooks/tensor.pt')\n",
    "                # print(attns.cpu().detach().numpy())\n",
    "            if not self.args.svd:\n",
    "                attns = attns.masked_fill(mask_matrix, 0)\n",
    "                attns = torch.mean(attns,dim=1)\n",
    "                atom_message = atom_message * attns.unsqueeze(-1)       # 需要再次加上w信息吗？\n",
    "                poly_vec = torch.sum(atom_message,dim=1) / torch.sum(attns,dim=1).unsqueeze(1)\n",
    "            else:\n",
    "                # pdb.set_trace()\n",
    "                atom_message[~mask.unsqueeze(-1).expand_as(atom_message)] = 0\n",
    "                U, S, V = torch.svd(atom_message, some=True, compute_uv=True)\n",
    "                poly_vec = torch.sum(V.transpose(-1,-2)[:,:10],dim=1)\n",
    "\n",
    "        else:\n",
    "        \"\"\"\n",
    "\n",
    "        return poly_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPN(nn.Module):\n",
    "    \"\"\"An :class:`MPN` is a wrapper around :class:`MPNEncoder` which featurizes input as needed.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: TrainArgs,\n",
    "                 atom_fdim: int = None,\n",
    "                 bond_fdim: int = None):\n",
    "        \"\"\"\n",
    "        :param args: A :class:`~chemprop.args.TrainArgs` object containing model arguments.\n",
    "        :param atom_fdim: Atom feature vector dimension.\n",
    "        :param bond_fdim: Bond feature vector dimension.\n",
    "        \"\"\"\n",
    "        super(MPN, self).__init__()\n",
    "        self.atom_fdim = atom_fdim or get_atom_fdim(overwrite_default_atom=args.overwrite_default_atom_features)\n",
    "        \n",
    "        self.bond_fdim = bond_fdim or get_bond_fdim(overwrite_default_atom=args.overwrite_default_atom_features,\n",
    "                                                    overwrite_default_bond=args.overwrite_default_bond_features,\n",
    "                                                    atom_messages=args.atom_messages)\n",
    "\n",
    "        self.features_only = args.features_only\n",
    "        self.use_input_features = args.use_input_features\n",
    "        self.device = args.device\n",
    "        self.atom_descriptors = args.atom_descriptors\n",
    "        self.overwrite_default_atom_features = args.overwrite_default_atom_features\n",
    "        self.overwrite_default_bond_features = args.overwrite_default_bond_features\n",
    "\n",
    "        if self.features_only:\n",
    "            return\n",
    "\n",
    "        if args.encoder_type in [\"wDMPNN\",\"wDMPNN_origin_attn\",\"wDMPNN_attn\",\"wDMPNN_pe\"]:\n",
    "            encoder = MPNEncoder\n",
    "        elif \"adj\" in args.encoder_type:\n",
    "            encoder = torch_adj_helper\n",
    "        else:\n",
    "            encoder = pyG_helper \n",
    "\n",
    "        if args.mpn_shared:\n",
    "            self.encoder = nn.ModuleList([encoder(args, self.atom_fdim, self.bond_fdim)] * args.number_of_molecules)\n",
    "        else:\n",
    "            self.encoder = nn.ModuleList([encoder(args, self.atom_fdim, self.bond_fdim)\n",
    "                                          for _ in range(args.number_of_molecules)])\n",
    "# 怎么最快的方式拿到数据\n",
    "    def forward(self,\n",
    "                batch: Union[List[List[str]], List[List[Chem.Mol]], List[List[Tuple[Chem.Mol, Chem.Mol]]], List[BatchMolGraph]],\n",
    "                features_batch: List[np.ndarray] = None,\n",
    "                atom_descriptors_batch: List[np.ndarray] = None,\n",
    "                atom_features_batch: List[np.ndarray] = None,\n",
    "                bond_features_batch: List[np.ndarray] = None,\n",
    "                # start_time=None,\n",
    "                # logger=None\n",
    "                ) -> torch.FloatTensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Encodes a batch of molecules.\n",
    "\n",
    "        :param batch: A list of list of SMILES, a list of list of RDKit molecules, or a\n",
    "                      list of :class:`~chemprop.features.featurization.BatchMolGraph`.\n",
    "                      The outer list or BatchMolGraph is of length :code:`num_molecules` (number of datapoints in batch),\n",
    "                      the inner list is of length :code:`number_of_molecules` (number of molecules per datapoint).\n",
    "        :param features_batch: A list of numpy arrays containing additional features.\n",
    "        :param atom_descriptors_batch: A list of numpy arrays containing additional atom descriptors.\n",
    "        :param atom_features_batch: A list of numpy arrays containing additional atom features.\n",
    "        :param bond_features_batch: A list of numpy arrays containing additional bond features.\n",
    "        :return: A PyTorch tensor of shape :code:`(num_molecules, hidden_size)` containing the encoding of each molecule.\n",
    "        \"\"\"\n",
    "        # debug = logger.debug if logger is not None else print\n",
    "        if type(batch[0]) != BatchMolGraph:\n",
    "            # Group first molecules, second molecules, etc for mol2graph\n",
    "            batch = [[mols[i] for mols in batch] for i in range(len(batch[0]))]\n",
    "\n",
    "            # TODO: handle atom_descriptors_batch with multiple molecules per input\n",
    "            if self.atom_descriptors == 'feature':\n",
    "                if len(batch) > 1:\n",
    "                    raise NotImplementedError('Atom/bond descriptors are currently only supported with one molecule '\n",
    "                                              'per input (i.e., number_of_molecules = 1).')\n",
    "\n",
    "                batch = [\n",
    "                    mol2graph(\n",
    "                        mols=b,\n",
    "                        atom_features_batch=atom_features_batch,\n",
    "                        bond_features_batch=bond_features_batch,\n",
    "                        overwrite_default_atom_features=self.overwrite_default_atom_features,\n",
    "                        overwrite_default_bond_features=self.overwrite_default_bond_features\n",
    "                    )\n",
    "                    for b in batch\n",
    "                ]\n",
    "            elif bond_features_batch is not None:\n",
    "                if len(batch) > 1:\n",
    "                    raise NotImplementedError('Atom/bond descriptors are currently only supported with one molecule '\n",
    "                                              'per input (i.e., number_of_molecules = 1).')\n",
    "\n",
    "                batch = [\n",
    "                    mol2graph(\n",
    "                        mols=b,\n",
    "                        bond_features_batch=bond_features_batch,\n",
    "                        overwrite_default_atom_features=self.overwrite_default_atom_features,\n",
    "                        overwrite_default_bond_features=self.overwrite_default_bond_features\n",
    "                    )\n",
    "                    for b in batch\n",
    "                ]\n",
    "            else:\n",
    "                batch = [mol2graph(b) for b in batch]\n",
    "        if self.use_input_features:\n",
    "            features_batch = torch.from_numpy(np.stack(features_batch)).float().to(self.device)\n",
    "\n",
    "            if self.features_only:\n",
    "                return features_batch\n",
    "\n",
    "        if self.atom_descriptors == 'descriptor':\n",
    "            if len(batch) > 1:\n",
    "                raise NotImplementedError('Atom descriptors are currently only supported with one molecule '\n",
    "                                          'per input (i.e., number_of_molecules = 1).')\n",
    "\n",
    "            encodings = [enc(ba, atom_descriptors_batch) for enc, ba in zip(self.encoder, batch)]\n",
    "        else:\n",
    "            \n",
    "            \n",
    "            encodings = [enc(ba) for enc, ba in zip(self.encoder, batch)]\n",
    "            \n",
    "            # 使用EGNN\n",
    "\n",
    "            \n",
    "\n",
    "        output = reduce(lambda x, y: torch.cat((x, y), dim=1), encodings)\n",
    "\n",
    "        if self.use_input_features:\n",
    "            if len(features_batch.shape) == 1:\n",
    "                features_batch = features_batch.view(1, -1)\n",
    "            output = torch.cat([output, features_batch], dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MoleculeModel(args)\n",
    "model.train()\n",
    "preds = model(\n",
    "        mol_batch,\n",
    "        features_batch,\n",
    "        atom_descriptors_batch,\n",
    "        atom_features_batch,\n",
    "        bond_descriptors_batch,\n",
    "        bond_features_batch,\n",
    "        constraints_batch,\n",
    "        bond_types_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pksgs/1/args.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('pksgs', str(1),'args.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
